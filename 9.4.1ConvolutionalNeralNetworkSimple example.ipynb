{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n",
    "</center>\n",
    "<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Learn Convolutional Neral Network</h5>\n",
    "<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li><a href=\"https://#ref0\">Helper functions </a></li>\n",
    "\n",
    "<li><a href=\"https://#ref1\"> Prepare Data </a></li>\n",
    "<li><a href=\"https://#ref2\">Convolutional Neral Network </a></li>\n",
    "<li><a href=\"https://#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n",
    "<li><a href=\"https://#ref4\">Analyse Results</a></li>\n",
    "\n",
    "<br>\n",
    "<p></p>\n",
    "Estimated Time Needed: <strong>25 min</strong>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "\n",
    "<h2 align=center>Helper functions </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2497920fe70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "\n",
    "<h2 align=center>Prepare Data </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x24900ddeac0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "\n",
    "### Build a Convolutional Neral Network Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "\n",
    "<h2 align=center>Analyse Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2021-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:\n",
    "\n",
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2021-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition.\n",
    "\n",
    "Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2021-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-23        | 2.0     | Srishti    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
